{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"MindSpore    HuggingFace","text":"<ul> <li> <p> Diffusers</p> <p>Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules.</p> <p> Be an artist now!</p> </li> <li> <p>\ud83e\udd17 Transformers</p> <p>Transformers provides thousands of pretrained models to perform tasks on different modalities such as text, vision, and audio.</p> <p> Autobots, transform, roll out!</p> </li> <li> <p>\ud83e\udd17 PEFT</p> <p>Parameter-Efficient Fine-Tuning (PEFT) methods enable efficient adaptation of large pretrained models to various downstream applications.</p> <p> Start tuning!</p> </li> <li> <p> Accelerate</p> <p>Accelerate is a library that enables the same MindSpore code to be run across any distributed configuration by adding just four lines of code.</p> <p> Launch! Launch! Launch!!!</p> </li> </ul>"},{"location":"#one-more-thing","title":"One more thing","text":"<p>Introducing Sora, an AI model that can create realistic and imaginative scenes from text instructions. Open source implementation from hpcai, pku.</p> <ul> <li> <p></p> <p>An aerial shot of a lighthouse standing tall on a rocky cliff, its beacon cutting through the early.</p> </li> <li> <p></p> <p>Drone shot along the Hawaii jungle coastline, sunny day. Kayaks in the water.</p> </li> <li> <p></p> <p>3D animation of a small, round, fluffy creature with big, expressive eyes explores a vibrant, enchan.</p> </li> </ul>"},{"location":"diffusers/","title":"\ud83e\udde8 Diffusers","text":"<p>State-of-the-art diffusion models for image and audio generation in MindSpore. We've tried to provide a completely consistent interface and usage with the huggingface/diffusers. Only necessary changes are made to the huggingface/diffusers to make it seamless for users from torch.</p> Info <p>Due to differences in framework, some APIs will not be identical to huggingface/diffusers in the foreseeable future, see Limitations for details.</p> <p>\ud83e\udd17 Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you're looking for a simple inference solution or want to train your own diffusion model, \ud83e\udd17 Diffusers is a modular toolbox that supports both. Our library is designed with a focus on usability over performance, simple over easy, and customizability over abstractions.</p> <p>The library has three main components:</p> <ul> <li>State-of-the-art diffusion pipelines for inference with just a few lines of code. There are many pipelines in \ud83e\udd17 Diffusers, check out the table in the pipeline overview for a complete list of available pipelines and the task they solve.</li> <li>Interchangeable noise schedulers for balancing trade-offs between generation speed and quality.</li> <li>Pretrained models that can be used as building blocks, and combined with schedulers, for creating your own end-to-end diffusion systems.</li> </ul> <ul> <li> <p>Tutorials</p> <p>Learn the fundamental skills you need to start generating outputs, build your own diffusion system, and train a diffusion model. We recommend starting here if you're using \ud83e\udd17 Diffusers for the first time!</p> </li> <li> <p>How-to guides</p> <p>Practical guides for helping you load pipelines, models, and schedulers. You'll also learn how to use pipelines for specific tasks, control how outputs are generated, optimize for inference speed, and different training techniques.</p> </li> <li> <p>Conceptual guides</p> <p>Understand why the library was designed the way it was, and learn more about the ethical guidelines and safety implementations for using the library.</p> </li> <li> <p>Reference</p> <p>Technical descriptions of how \ud83e\udd17 Diffusers classes and methods work.</p> </li> </ul>"},{"location":"diffusers/#make-dffusers-run-on-mindspore","title":"Make \ud83e\udd17 D\ud83e\udde8ffusers Run on MindSpore","text":""},{"location":"diffusers/installation/","title":"Installation","text":""},{"location":"diffusers/installation/#installation","title":"Installation","text":"<p>\ud83e\udd17 Diffusers is tested on Python 3.8+, MindSpore 2.3+. Follow the installation instructions below for the deep learning library you are using:</p> <ul> <li>MindSpore installation instructions</li> </ul>"},{"location":"diffusers/installation/#install-with-pip","title":"Install with pip","text":"<p>You should install \ud83e\udd17 Diffusers in a virtual environment. If you're unfamiliar with Python virtual environments, take a look at this guide. A virtual environment makes it easier to manage different projects and avoid compatibility issues between dependencies.</p> <p>Start by creating a virtual environment in your project directory:</p> <pre><code>python -m venv .env\n</code></pre> <p>Activate the virtual environment:</p> <pre><code>source .env/bin/activate\n</code></pre> <p>You should also install \ud83e\udd17 Transformers because \ud83e\udd17 Diffusers relies on its models:</p> <pre><code>pip install mindone transformers\n</code></pre>"},{"location":"diffusers/installation/#install-from-source","title":"Install from source","text":"<p>Before installing \ud83e\udd17 Diffusers from source, make sure you have MindSpore installed.</p> <p>Then install \ud83e\udd17 Diffusers from source:</p> <pre><code>pip install git+https://github.com/mindspore-lab/mindone\n</code></pre> <p>This command installs the bleeding edge <code>main</code> version rather than the latest <code>stable</code> version. The <code>main</code> version is useful for staying up-to-date with the latest developments. For instance, if a bug has been fixed since the last official release but a new release hasn't been rolled out yet. However, this means the <code>main</code> version may not always be stable. We strive to keep the <code>main</code> version operational, and most issues are usually resolved within a few hours or a day. If you run into a problem, please open an Issue so we can fix it even sooner!</p>"},{"location":"diffusers/installation/#editable-install","title":"Editable install","text":"<p>You will need an editable install if you'd like to:</p> <ul> <li>Use the <code>main</code> version of the source code.</li> <li>Contribute to \ud83e\udd17 Diffusers and need to test changes in the code.</li> </ul> <p>Clone the repository and install \ud83e\udd17 Diffusers with the following commands:</p> <pre><code>git clone https://github.com/mindspore-lab/mindone.git\ncd mindone\n</code></pre> <pre><code>pip install -e .\n</code></pre> <p>These commands will link the folder you cloned the repository to and your Python library paths. Python will now look inside the folder you cloned to in addition to the normal library paths. For example, if your Python packages are typically installed in <code>~/anaconda3/envs/main/lib/python3.8/site-packages/</code>, Python will also search the <code>~/mindone/</code> folder you cloned to.</p> <p>Warning</p> <p>You must keep the <code>mindone</code> folder if you want to keep using the library.</p> <p>Now you can easily update your clone to the latest version of \ud83e\udd17 Diffusers with the following command:</p> <pre><code>cd ~/mindone/\ngit pull\n</code></pre> <p>Your Python environment will find the <code>main</code> version of \ud83e\udd17 Diffusers on the next run.</p>"},{"location":"diffusers/installation/#cache","title":"Cache","text":"<p>Model weights and files are downloaded from the Hub to a cache which is usually your home directory. You can change the cache location by specifying the <code>HF_HOME</code> or <code>HUGGINFACE_HUB_CACHE</code> environment variables or configuring the <code>cache_dir</code> parameter in methods like [<code>~DiffusionPipeline.from_pretrained</code>].</p> <p>Cached files allow you to run \ud83e\udd17 Diffusers offline. To prevent \ud83e\udd17 Diffusers from connecting to the internet, set the <code>HF_HUB_OFFLINE</code> environment variable to <code>True</code> and \ud83e\udd17 Diffusers will only load previously downloaded files in the cache.</p> <pre><code>export HF_HUB_OFFLINE=True\n</code></pre> <p>For more details about managing and cleaning the cache, take a look at the caching guide.</p>"},{"location":"diffusers/installation/#telemetry-logging","title":"Telemetry logging","text":"<p>Our library gathers telemetry information during [<code>~DiffusionPipeline.from_pretrained</code>] requests. The data gathered includes the version of \ud83e\udd17 Diffusers and PyTorch/Flax, the requested model or pipeline class, and the path to a pretrained checkpoint if it is hosted on the Hugging Face Hub. This usage data helps us debug issues and prioritize new features. Telemetry is only sent when loading models and pipelines from the Hub, and it is not collected if you're loading local files.</p> <p>We understand that not everyone wants to share additional information,and we respect your privacy. You can disable telemetry collection by setting the <code>DISABLE_TELEMETRY</code> environment variable from your terminal:</p> <p>On Linux/MacOS: <pre><code>export DISABLE_TELEMETRY=YES\n</code></pre></p> <p>On Windows: <pre><code>set DISABLE_TELEMETRY=YES\n</code></pre></p>"},{"location":"diffusers/limitations/","title":"Limitations","text":"<p>Due to differences in framework, some APIs &amp; models will not be identical to huggingface/diffusers in the foreseeable future.</p>"},{"location":"diffusers/limitations/#apis","title":"APIs","text":""},{"location":"diffusers/limitations/#xxxfrom_pretrained","title":"<code>xxx.from_pretrained</code>","text":"<ul> <li><code>torch_dtype</code> is renamed to <code>mindspore_dtype</code></li> <li><code>device_map</code>, <code>max_memory</code>, <code>offload_folder</code>, <code>offload_state_dict</code>, <code>low_cpu_mem_usage</code> will not be supported.</li> </ul>"},{"location":"diffusers/limitations/#baseoutput","title":"<code>BaseOutput</code>","text":"<ul> <li>Default value of <code>return_dict</code> is changed to <code>False</code>, for <code>GRAPH_MODE</code> does not allow to construct an instance of it.</li> </ul>"},{"location":"diffusers/limitations/#output-of-autoencoderklencode","title":"Output of <code>AutoencoderKL.encode</code>","text":"<p>Unlike the output <code>posterior = DiagonalGaussianDistribution(latent)</code>, which can do sampling by <code>posterior.sample()</code>. We can only output the <code>latent</code> and then do sampling through <code>AutoencoderKL.diag_gauss_dist.sample(latent)</code>.</p>"},{"location":"diffusers/limitations/#selfconfig-in-construct","title":"<code>self.config</code> in <code>construct()</code>","text":"<p>For many models, parameters used in initialization will be registered in <code>self.config</code>. They are often accessed during the <code>construct</code> like using <code>if self.config.xxx == xxx</code> to determine execution paths in origin \ud83e\udd17diffusers. However getting attributes like this is not supported by static graph syntax of MindSpore. Two feasible replacement options are</p> <ul> <li>set new attributes in initialization for <code>self</code> like <code>self.xxx = self.config.xxx</code>, then use <code>self.xxx</code> in <code>construct</code> instead.</li> <li>use <code>self.config[\"xxx\"]</code> as <code>self.config</code> is an <code>OrderedDict</code> and getting items like this is supported in static graph mode.</li> </ul> <p>When <code>self.config.xxx</code> changed, we change <code>self.xxx</code> and <code>self.config[\"xxx\"]</code> both.</p>"},{"location":"diffusers/limitations/#models","title":"Models","text":"<p>The table below represents the current support in mindone/diffusers for each of those modules, whether they have support in Pynative fp16 mode, Graph fp16 mode, Pynative fp32 mode or Graph fp32 mode.</p> Names Pynative FP16 Pynative FP32 Graph FP16 Graph FP32 Description StableCascadeUNet \u274c \u2705 \u274c \u2705 huggingface/diffusers output NaN when using float16. nn.Conv3d \u2705 \u274c \u2705 \u274c FP32 is not supported on Ascend TemporalConvLayer \u2705 \u274c \u2705 \u274c contains nn.Conv3d TemporalResnetBlock \u2705 \u274c \u2705 \u274c contains nn.Conv3d SpatioTemporalResBlock \u2705 \u274c \u2705 \u274c contains TemporalResnetBlock UNetMidBlock3DCrossAttn \u2705 \u274c \u2705 \u274c contains TemporalConvLayer CrossAttnDownBlock3D \u2705 \u274c \u2705 \u274c contains TemporalConvLayer DownBlock3D \u2705 \u274c \u2705 \u274c contains TemporalConvLayer CrossAttnUpBlock3D \u2705 \u274c \u2705 \u274c contains TemporalConvLayer UpBlock3D \u2705 \u274c \u2705 \u274c contains TemporalConvLayer MidBlockTemporalDecoder \u2705 \u274c \u2705 \u274c contains SpatioTemporalResBlock UpBlockTemporalDecoder \u2705 \u274c \u2705 \u274c contains SpatioTemporalResBlock UNetMidBlockSpatioTemporal \u2705 \u274c \u2705 \u274c contains SpatioTemporalResBlock DownBlockSpatioTemporal \u2705 \u274c \u2705 \u274c contains SpatioTemporalResBlock CrossAttnDownBlockSpatioTemporal \u2705 \u274c \u2705 \u274c contains SpatioTemporalResBlock UpBlockSpatioTemporal \u2705 \u274c \u2705 \u274c contains SpatioTemporalResBlock CrossAttnUpBlockSpatioTemporal \u2705 \u274c \u2705 \u274c contains SpatioTemporalResBlock TemporalDecoder \u2705 \u274c \u2705 \u274c contains nn.Conv3d, MidBlockTemporalDecoder etc. UNet3DConditionModel \u2705 \u274c \u2705 \u274c contains UNetMidBlock3DCrossAttn etc. I2VGenXLUNet \u2705 \u274c \u2705 \u274c contains UNetMidBlock3DCrossAttn etc. AutoencoderKLTemporalDecoder \u2705 \u274c \u2705 \u274c contains MidBlockTemporalDecoder etc. UNetSpatioTemporalConditionModel \u2705 \u274c \u2705 \u274c contains UNetMidBlockSpatioTemporal etc. FirUpsample2D \u274c \u2705 \u2705 \u2705 ops.Conv2D has poor precision in fp16 and PyNative mode FirDownsample2D \u274c \u2705 \u2705 \u2705 ops.Conv2D has poor precision in fp16 and PyNative mode AttnSkipUpBlock2D \u274c \u2705 \u2705 \u2705 contains FirUpsample2D SkipUpBlock2D \u274c \u2705 \u2705 \u2705 contains FirUpsample2D AttnSkipDownBlock2D \u274c \u2705 \u2705 \u2705 contains FirDownsample2D SkipDownBlock2D \u274c \u2705 \u2705 \u2705 contains FirDownsample2D ResnetBlock2D (kernel='fir') \u274c \u2705 \u2705 \u2705 ops.Conv2D has poor precision in fp16 and PyNative mode"},{"location":"diffusers/limitations/#pipelines","title":"Pipelines","text":"<p>The table below represents the current support in mindone/diffusers for each of those pipelines in MindSpore 2.3.0, whether they have support in Pynative fp16 mode, Graph fp16 mode, Pynative fp32 mode or Graph fp32 mode.</p> <p>precision issues of pipelines, the experiments in the table below default to upcasting GroupNorm to FP32 to avoid this issue.</p> Pipelines Pynative FP16 Pynative FP32 Graph FP16 Graph FP32 Description AnimateDiffPipeline AnimateDiffVideoToVideoPipeline In FP32 and Pynative mode, this pipeline will run out of memory BlipDiffusionPipeline ConsistencyModelPipeline DDIMPipeline DDPMPipeline DiTPipeline I2VGenXLPipeline ops.bmm and ops.softmax have precision issues under FP16, so we need to upcast them to FP32 to get a good result IFImg2ImgPipeline IFImg2ImgSuperResolutionPipeline IFInpaintingPipeline IFInpaintingSuperResolutionPipeline IFPipeline IFSuperResolutionPipeline Kandinsky3Img2ImgPipeline Kandinsky3 only provides FP16 weights; additionally, T5 has precision issues, so to achieve the desired results, you need to directly input prompt_embeds and attention_mask. Kandinsky3Pipeline Kandinsky3 only provides FP16 weights; additionally, T5 has precision issues, so to achieve the desired results, you need to directly input prompt_embeds and attention_mask. KandinskyImg2ImgPipeline KandinskyInpaintPipeline KandinskyPipeline KandinskyV22ControlnetImg2ImgPipeline KandinskyV22ControlnetPipeline KandinskyV22Img2ImgPipeline KandinskyV22InpaintPipeline KandinskyV22Pipeline LatentConsistencyModelImg2ImgPipeline LatentConsistencyModelPipeline LDMSuperResolutionPipeline LDMTextToImagePipeline PixArtAlphaPipeline ShapEImg2ImgPipeline The syntax in Render only supports Pynative mode ShapEPipeline The syntax in Render only supports Pynative mode StableCascadePipeline This pipeline does not support FP16 due to precision issues StableDiffusion3Pipeline StableDiffusionAdapterPipeline StableDiffusionControlNetImg2ImgPipeline StableDiffusionControlNetInpaintPipeline StableDiffusionControlNetPipeline StableDiffusionDepth2ImgPipeline StableDiffusionDiffEditPipeline StableDiffusionGLIGENPipeline StableDiffusionGLIGENTextImagePipeline StableDiffusionImageVariationPipeline StableDiffusionImg2ImgPipeline StableDiffusionInpaintPipeline StableDiffusionInstructPix2PixPipeline StableDiffusionLatentUpscalePipeline StableDiffusionPipeline StableDiffusionUpscalePipeline StableDiffusionXLAdapterPipeline StableDiffusionXLControlNetImg2ImgPipeline StableDiffusionXLControlNetInpaintPipeline StableDiffusionXLControlNetPipeline StableDiffusionXLImg2ImgPipeline StableDiffusionXLInpaintPipeline StableDiffusionXLInstructPix2PixPipeline StableDiffusionXLPipeline StableVideoDiffusionPipeline This pipeline will run out of memory under FP32; ops.bmm and ops.softmax have precision issues under FP16, so we need to upcast them to FP32 to get a good result UnCLIPImageVariationPipeline UnCLIPPipeline WuerstchenPipeline GlobalResponseNorm has precision issue under FP16, so we need to upcast it to FP32 to get a good result"},{"location":"diffusers/quicktour/","title":"Quicktour","text":""},{"location":"diffusers/quicktour/#quicktour","title":"Quicktour","text":"<p>Diffusion models are trained to denoise random Gaussian noise step-by-step to generate a sample of interest, such as an image or audio. This has sparked a tremendous amount of interest in generative AI, and you have probably seen examples of diffusion generated images on the internet. \ud83e\udde8 Diffusers is a library aimed at making diffusion models widely accessible to everyone.</p> <p>Whether you're a developer or an everyday user, this quicktour will introduce you to \ud83e\udde8 Diffusers and help you get up and generating quickly! There are three main components of the library to know about:</p> <ul> <li>The [<code>DiffusionPipeline</code>] is a high-level end-to-end class designed to rapidly generate samples from pretrained diffusion models for inference.</li> <li>Popular pretrained model architectures and modules that can be used as building blocks for creating diffusion systems.</li> <li>Many different schedulers - algorithms that control how noise is added for training, and how to generate denoised images during inference.</li> </ul> <p>The quicktour will show you how to use the [<code>DiffusionPipeline</code>] for inference, and then walk you through how to combine a model and scheduler to replicate what's happening inside the [<code>DiffusionPipeline</code>].</p> <p>Tip</p> <p>The quicktour is a simplified version of the introductory \ud83e\udde8 Diffusers notebook to help you get started quickly. If you want to learn more about \ud83e\udde8 Diffusers' goal, design philosophy, and additional details about its core API, check out the notebook!</p> <p>Before you begin, make sure you have all the necessary libraries installed:</p> <pre><code># uncomment to install the necessary libraries in Colab\n#!pip install --upgrade mindone\n</code></pre> <ul> <li>\ud83e\udd17 Accelerate speeds up model loading for inference and training.</li> <li>\ud83e\udd17 Transformers is required to run the most popular diffusion models, such as Stable Diffusion.</li> </ul>"},{"location":"diffusers/quicktour/#diffusionpipeline","title":"DiffusionPipeline","text":"<p>The [<code>DiffusionPipeline</code>] is the easiest way to use a pretrained diffusion system for inference. It is an end-to-end system containing the model and the scheduler. You can use the [<code>DiffusionPipeline</code>] out-of-the-box for many tasks. Take a look at the table below for some supported tasks, and for a complete list of supported tasks, check out the \ud83e\udde8 Diffusers Summary table.</p> Task Description Pipeline Unconditional Image Generation generate an image from Gaussian noise unconditional_image_generation Text-Guided Image Generation generate an image given a text prompt conditional_image_generation Text-Guided Image-to-Image Translation adapt an image guided by a text prompt img2img Text-Guided Image-Inpainting fill the masked part of an image given the image, the mask and a text prompt inpaint Text-Guided Depth-to-Image Translation adapt parts of an image guided by a text prompt while preserving structure via depth estimation depth2img <p>Start by creating an instance of a [<code>DiffusionPipeline</code>] and specify which pipeline checkpoint you would like to download. You can use the [<code>DiffusionPipeline</code>] for any checkpoint stored on the Hugging Face Hub. In this quicktour, you'll load the <code>stable-diffusion-v1-5</code> checkpoint for text-to-image generation.</p> <p>Warning</p> <p>For Stable Diffusion models, please carefully read the license first before running the model. \ud83e\udde8 Diffusers implements a <code>safety_checker</code> to prevent offensive or harmful content, but the model's improved image generation capabilities can still produce potentially harmful content.</p> <p>Load the model with the [<code>~DiffusionPipeline.from_pretrained</code>] method:</p> <pre><code>- &gt;&gt;&gt; from diffusers import DiffusionPipeline\n+ &gt;&gt;&gt; from mindone.diffusers import DiffusionPipeline\n\n  &gt;&gt;&gt; pipeline = DiffusionPipeline.from_pretrained(\n  ...     \"runwayml/stable-diffusion-v1-5\",\n- ...     torch_dtype=torch.float32,\n+ ...     mindspore_dtype=mindspore.float32\n  ...     use_safetensors=True\n  ... )\n</code></pre> <p>The [<code>DiffusionPipeline</code>] downloads and caches all modeling, tokenization, and scheduling components. You'll see that the Stable Diffusion pipeline is composed of the [<code>UNet2DConditionModel</code>] and [<code>PNDMScheduler</code>] among other things:</p> <pre><code>&gt;&gt;&gt; pipeline\nStableDiffusionPipeline {\n  \"_class_name\": \"StableDiffusionPipeline\",\n  \"_diffusers_version\": \"0.21.4\",\n  ...,\n  \"scheduler\": [\n    \"diffusers\",\n    \"PNDMScheduler\"\n  ],\n  ...,\n  \"unet\": [\n    \"diffusers\",\n    \"UNet2DConditionModel\"\n  ],\n  \"vae\": [\n    \"diffusers\",\n    \"AutoencoderKL\"\n  ]\n}\n</code></pre> <p>We strongly recommend running the pipeline on a GPU because the model consists of roughly 1.4 billion parameters. You can't move the generator object to a GPU manually, because MindSpore implicitly does that. Do NOT invoke <code>to(\"cuda\")</code>:</p> <pre><code>- &gt;&gt;&gt; pipeline.to(\"cuda\")\n</code></pre> <p>Now you can pass a text prompt to the <code>pipeline</code> to generate an image, and then access the denoised image. By default, the image output is wrapped in a <code>PIL.Image</code> object.</p> <pre><code>&gt;&gt;&gt; image = pipeline(\"An image of a squirrel in Picasso style\")[0][0]\n&gt;&gt;&gt; image\n</code></pre> <p>Save the image by calling <code>save</code>:</p> <pre><code>&gt;&gt;&gt; image.save(\"image_of_squirrel_painting.png\")\n</code></pre>"},{"location":"diffusers/quicktour/#local-pipeline","title":"Local pipeline","text":"<p>You can also use the pipeline locally. The only difference is you need to download the weights first:</p> <pre><code>!git lfs install\n!git clone https://huggingface.co/runwayml/stable-diffusion-v1-5\n</code></pre> <p>Then load the saved weights into the pipeline:</p> <pre><code>&gt;&gt;&gt; pipeline = DiffusionPipeline.from_pretrained(\"./stable-diffusion-v1-5\", use_safetensors=True)\n</code></pre> <p>Now, you can run the pipeline as you would in the section above.</p>"},{"location":"diffusers/quicktour/#swapping-schedulers","title":"Swapping schedulers","text":"<p>Different schedulers come with different denoising speeds and quality trade-offs. The best way to find out which one works best for you is to try them out! One of the main features of \ud83e\udde8 Diffusers is to allow you to easily switch between schedulers. For example, to replace the default [<code>PNDMScheduler</code>] with the [<code>EulerDiscreteScheduler</code>], load it with the [<code>~diffusers.ConfigMixin.from_config</code>] method:</p> <pre><code>&gt;&gt;&gt; from mindone.diffusers import EulerDiscreteScheduler\n\n&gt;&gt;&gt; pipeline = DiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", use_safetensors=True)\n&gt;&gt;&gt; pipeline.scheduler = EulerDiscreteScheduler.from_config(pipeline.scheduler.config)\n</code></pre> <p>Try generating an image with the new scheduler and see if you notice a difference!</p> <p>In the next section, you'll take a closer look at the components - the model and scheduler - that make up the [<code>DiffusionPipeline</code>] and learn how to use these components to generate an image of a cat.</p>"},{"location":"diffusers/quicktour/#models","title":"Models","text":"<p>Most models take a noisy sample, and at each timestep it predicts the noise residual (other models learn to predict the previous sample directly or the velocity or <code>v-prediction</code>), the difference between a less noisy image and the input image. You can mix and match models to create other diffusion systems.</p> <p>Models are initiated with the [<code>~ModelMixin.from_pretrained</code>] method which also locally caches the model weights so it is faster the next time you load the model. For the quicktour, you'll load the [<code>UNet2DModel</code>], a basic unconditional image generation model with a checkpoint trained on cat images:</p> <pre><code>&gt;&gt;&gt; from mindone.diffusers import UNet2DModel\n\n&gt;&gt;&gt; repo_id = \"google/ddpm-cat-256\"\n&gt;&gt;&gt; model = UNet2DModel.from_pretrained(repo_id, use_safetensors=True)\n</code></pre> <p>To access the model parameters, call <code>model.config</code>:</p> <pre><code>&gt;&gt;&gt; model.config\n</code></pre> <p>The model configuration is a \ud83e\uddca frozen \ud83e\uddca dictionary, which means those parameters can't be changed after the model is created. This is intentional and ensures that the parameters used to define the model architecture at the start remain the same, while other parameters can still be adjusted during inference.</p> <p>Some of the most important parameters are:</p> <ul> <li><code>sample_size</code>: the height and width dimension of the input sample.</li> <li><code>in_channels</code>: the number of input channels of the input sample.</li> <li><code>down_block_types</code> and <code>up_block_types</code>: the type of down- and upsampling blocks used to create the UNet architecture.</li> <li><code>block_out_channels</code>: the number of output channels of the downsampling blocks; also used in reverse order for the number of input channels of the upsampling blocks.</li> <li><code>layers_per_block</code>: the number of ResNet blocks present in each UNet block.</li> </ul> <p>To use the model for inference, create the image shape with random Gaussian noise. It should have a <code>batch</code> axis because the model can receive multiple random noises, a <code>channel</code> axis corresponding to the number of input channels, and a <code>sample_size</code> axis for the height and width of the image:</p> <pre><code>&gt;&gt;&gt; import mindspore\n\n&gt;&gt;&gt; noisy_sample = mindspore.ops.randn(1, model.config.in_channels, model.config.sample_size, model.config.sample_size)\n&gt;&gt;&gt; noisy_sample.shape\n[1, 3, 256, 256]\n</code></pre> <p>For inference, pass the noisy image and a <code>timestep</code> to the model. The <code>timestep</code> indicates how noisy the input image is, with more noise at the beginning and less at the end. This helps the model determine its position in the diffusion process, whether it is closer to the start or the end. Use the <code>sample</code> method to get the model output:</p> <pre><code>&gt;&gt;&gt; noisy_residual = model(sample=noisy_sample, timestep=2)[0]\n</code></pre> <p>To generate actual examples though, you'll need a scheduler to guide the denoising process. In the next section, you'll learn how to couple a model with a scheduler.</p>"},{"location":"diffusers/quicktour/#schedulers","title":"Schedulers","text":"<p>Schedulers manage going from a noisy sample to a less noisy sample given the model output - in this case, it is the <code>noisy_residual</code>.</p> <p>Tip</p> <p>\ud83e\udde8 Diffusers is a toolbox for building diffusion systems. While the [<code>DiffusionPipeline</code>] is a convenient way to get started with a pre-built diffusion system, you can also choose your own model and scheduler components separately to build a custom diffusion system.</p> <p>For the quicktour, you'll instantiate the [<code>DDPMScheduler</code>] with its [<code>~diffusers.ConfigMixin.from_config</code>] method:</p> <pre><code>&gt;&gt;&gt; from mindone.diffusers import DDPMScheduler\n\n&gt;&gt;&gt; scheduler = DDPMScheduler.from_pretrained(repo_id)\n&gt;&gt;&gt; scheduler\nDDPMScheduler {\n  \"_class_name\": \"DDPMScheduler\",\n  \"_diffusers_version\": \"0.21.4\",\n  \"beta_end\": 0.02,\n  \"beta_schedule\": \"linear\",\n  \"beta_start\": 0.0001,\n  \"clip_sample\": true,\n  \"clip_sample_range\": 1.0,\n  \"dynamic_thresholding_ratio\": 0.995,\n  \"num_train_timesteps\": 1000,\n  \"prediction_type\": \"epsilon\",\n  \"sample_max_value\": 1.0,\n  \"steps_offset\": 0,\n  \"thresholding\": false,\n  \"timestep_spacing\": \"leading\",\n  \"trained_betas\": null,\n  \"variance_type\": \"fixed_small\"\n}\n</code></pre> <p>Tip</p> <p>\ud83d\udca1 Unlike a model, a scheduler does not have trainable weights and is parameter-free!</p> <p>Some of the most important parameters are:</p> <ul> <li><code>num_train_timesteps</code>: the length of the denoising process or, in other words, the number of timesteps required to process random Gaussian noise into a data sample.</li> <li><code>beta_schedule</code>: the type of noise schedule to use for inference and training.</li> <li><code>beta_start</code> and <code>beta_end</code>: the start and end noise values for the noise schedule.</li> </ul> <p>To predict a slightly less noisy image, pass the following to the scheduler's [<code>~diffusers.DDPMScheduler.step</code>] method: model output, <code>timestep</code>, and current <code>sample</code>.</p> <pre><code>&gt;&gt;&gt; less_noisy_sample = scheduler.step(model_output=noisy_residual, timestep=2, sample=noisy_sample)[0]\n&gt;&gt;&gt; less_noisy_sample.shape\n[1, 3, 256, 256]\n</code></pre> <p>The <code>less_noisy_sample</code> can be passed to the next <code>timestep</code> where it'll get even less noisy! Let's bring it all together now and visualize the entire denoising process.</p> <p>First, create a function that postprocesses and displays the denoised image as a <code>PIL.Image</code>:</p> <pre><code>&gt;&gt;&gt; import PIL.Image\n&gt;&gt;&gt; import numpy as np\n\n\n&gt;&gt;&gt; def display_sample(sample, i):\n...     image_processed = sample.permute(0, 2, 3, 1)\n...     image_processed = (image_processed + 1.0) * 127.5\n...     image_processed = image_processed.numpy().astype(np.uint8)\n...\n...     image_pil = PIL.Image.fromarray(image_processed[0])\n...     display(f\"Image at step {i}\")\n...     display(image_pil)\n</code></pre> <p>Now create a denoising loop that predicts the residual of the less noisy sample, and computes the less noisy sample with the scheduler:</p> <pre><code>&gt;&gt;&gt; import tqdm\n\n&gt;&gt;&gt; sample = noisy_sample\n\n&gt;&gt;&gt; for i, t in enumerate(tqdm.tqdm(scheduler.timesteps)):\n...     # 1. predict noise residual\n...     residual = model(sample, t)[0]\n...\n...     # 2. compute less noisy image and set x_t -&gt; x_t-1\n...     sample = scheduler.step(residual, t, sample)[0]\n...\n...     # 3. optionally look at image\n...     if (i + 1) % 50 == 0:\n...         display_sample(sample, i + 1)\n</code></pre> <p>Sit back and watch as a cat is generated from nothing but noise! \ud83d\ude3b</p>"},{"location":"diffusers/quicktour/#next-steps","title":"Next steps","text":"<p>Hopefully, you generated some cool images with \ud83e\udde8 Diffusers in this quicktour! For your next steps, you can:</p> <ul> <li>Train or finetune a model to generate your own images in the training tutorial.</li> <li>See example official and community training or finetuning scripts for a variety of use cases.</li> <li>Learn more about loading, accessing, changing, and comparing schedulers in the Using different Schedulers guide.</li> <li>Explore prompt engineering, speed and memory optimizations, and tips and tricks for generating higher-quality images with the Stable Diffusion guide.</li> <li>Dive deeper into speeding up \ud83e\udde8 Diffusers with guides on optimized MindSpore on a NPU.</li> </ul>"},{"location":"diffusers/stable_diffusion/","title":"Effective and efficient diffusion","text":""},{"location":"diffusers/stable_diffusion/#effective-and-efficient-diffusion","title":"Effective and efficient diffusion","text":"<p>Getting the [<code>DiffusionPipeline</code>] to generate images in a certain style or include what you want can be tricky. Often times, you have to run the [<code>DiffusionPipeline</code>] several times before you end up with an image you're happy with. But generating something out of nothing is a computationally intensive process, especially if you're running inference over and over again.</p> <p>This is why it's important to get the most computational (speed) and memory (GPU vRAM) efficiency from the pipeline to reduce the time between inference cycles so you can iterate faster.</p> <p>This tutorial walks you through how to generate faster and better with the [<code>DiffusionPipeline</code>].</p> <p>Begin by loading the <code>runwayml/stable-diffusion-v1-5</code> model:</p> <pre><code>from mindone.diffusers import DiffusionPipeline\n\nmodel_id = \"runwayml/stable-diffusion-v1-5\"\npipeline = DiffusionPipeline.from_pretrained(model_id, use_safetensors=True)\n</code></pre> <p>The example prompt you'll use is a portrait of an old warrior chief, but feel free to use your own prompt:</p> <pre><code>prompt = \"portrait photo of a old warrior chief\"\n</code></pre>"},{"location":"diffusers/stable_diffusion/#speed","title":"Speed","text":"<p>One of the simplest ways to speed up inference is to place the pipeline on a GPU the same way you would with any Mindspore cell. That is, do nothing! MindSpore will automatically take care of model placement, so you don't need to:</p> <pre><code>- pipeline = pipeline.to(\"cuda\")\n</code></pre> <p>To make sure you can use the same image and improve on it, use a <code>Generator</code> and set a seed for reproducibility:</p> <pre><code>import numpy as np\n\ngenerator = np.random.Generator(np.random.PCG64(seed=0))\n</code></pre> <p>Now you can generate an image:</p> <pre><code>image = pipeline(prompt, generator=generator)[0][0]\nimage\n</code></pre> <p>This process took ~30 seconds on a T4 GPU (it might be faster if your allocated GPU is better than a T4). By default, the [<code>DiffusionPipeline</code>] runs inference with full <code>float32</code> precision for 50 inference steps. You can speed this up by switching to a lower precision like <code>float16</code> or running fewer inference steps.</p> <p>Let's start by loading the model in <code>float16</code> and generate an image:</p> <pre><code>import mindspore\n\npipeline = DiffusionPipeline.from_pretrained(model_id, mindspore_dtype=mindspore.float16, use_safetensors=True)\ngenerator = np.random.Generator(np.random.PCG64(seed=0))\nimage = pipeline(prompt, generator=generator)[0][0]\nimage\n</code></pre> <p>This time, it only took ~11 seconds to generate the image, which is almost 3x faster than before!</p> <p>Tip</p> <p>\ud83d\udca1 We strongly suggest always running your pipelines in <code>float16</code>, and so far, we've rarely seen any degradation in output quality.</p> <p>Another option is to reduce the number of inference steps. Choosing a more efficient scheduler could help decrease the number of steps without sacrificing output quality. You can find which schedulers are compatible with the current model in the [<code>DiffusionPipeline</code>] by calling the <code>compatibles</code> method:</p> <pre><code>pipeline.scheduler.compatibles\n[\n    diffusers.schedulers.scheduling_lms_discrete.LMSDiscreteScheduler,\n    diffusers.schedulers.scheduling_unipc_multistep.UniPCMultistepScheduler,\n    diffusers.schedulers.scheduling_k_dpm_2_discrete.KDPM2DiscreteScheduler,\n    diffusers.schedulers.scheduling_deis_multistep.DEISMultistepScheduler,\n    diffusers.schedulers.scheduling_euler_discrete.EulerDiscreteScheduler,\n    diffusers.schedulers.scheduling_dpmsolver_multistep.DPMSolverMultistepScheduler,\n    diffusers.schedulers.scheduling_ddpm.DDPMScheduler,\n    diffusers.schedulers.scheduling_dpmsolver_singlestep.DPMSolverSinglestepScheduler,\n    diffusers.schedulers.scheduling_k_dpm_2_ancestral_discrete.KDPM2AncestralDiscreteScheduler,\n    diffusers.utils.dummy_torch_and_torchsde_objects.DPMSolverSDEScheduler,\n    diffusers.schedulers.scheduling_heun_discrete.HeunDiscreteScheduler,\n    diffusers.schedulers.scheduling_pndm.PNDMScheduler,\n    diffusers.schedulers.scheduling_euler_ancestral_discrete.EulerAncestralDiscreteScheduler,\n    diffusers.schedulers.scheduling_ddim.DDIMScheduler,\n]\n</code></pre> <p>The Stable Diffusion model uses the [<code>PNDMScheduler</code>] by default which usually requires ~50 inference steps, but more performant schedulers like [<code>DPMSolverMultistepScheduler</code>], require only ~20 or 25 inference steps. Use the [<code>~ConfigMixin.from_config</code>] method to load a new scheduler:</p> <pre><code>from mindone.diffusers import DPMSolverMultistepScheduler\n\npipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config)\n</code></pre> <p>Now set the <code>num_inference_steps</code> to 20:</p> <pre><code>generator = np.random.Generator(np.random.PCG64(seed=0))\nimage = pipeline(prompt, generator=generator, num_inference_steps=20)[0][0]\nimage\n</code></pre> <p>Great, you've managed to cut the inference time to just 4 seconds! \u26a1\ufe0f</p>"},{"location":"diffusers/stable_diffusion/#memory","title":"Memory","text":"<p>The other key to improving pipeline performance is consuming less memory, which indirectly implies more speed, since you're often trying to maximize the number of images generated per second. The easiest way to see how many images you can generate at once is to try out different batch sizes until you get an <code>OutOfMemoryError</code> (OOM).</p> <p>Create a function that'll generate a batch of images from a list of prompts and <code>Generators</code>. Make sure to assign each <code>Generator</code> a seed so you can reuse it if it produces a good result.</p> <pre><code>def get_inputs(batch_size=1):\n    generator = [np.random.Generator(np.random.PCG64(seed=i)) for i in range(batch_size)]\n    prompts = batch_size * [prompt]\n    num_inference_steps = 20\n\n    return {\"prompt\": prompts, \"generator\": generator, \"num_inference_steps\": num_inference_steps}\n</code></pre> <p>Start with <code>batch_size=4</code> and see how much memory you've consumed:</p> <pre><code>from mindone.diffusers.utils import make_image_grid\n\nimages = pipeline(**get_inputs(batch_size=4))[0]\nmake_image_grid(images, 2, 2)\n</code></pre> <p>Unless you have a GPU with more vRAM, the code above probably returned an <code>OOM</code> error! Most of the memory is taken up by the cross-attention layers. Instead of running this operation in a batch, you can run it sequentially to save a significant amount of memory. All you have to do is configure the pipeline to use the [<code>~DiffusionPipeline.enable_attention_slicing</code>] function:</p> <pre><code>pipeline.enable_attention_slicing()\n</code></pre> <p>Now try increasing the <code>batch_size</code> to 8!</p> <pre><code>images = pipeline(**get_inputs(batch_size=8)).images\nmake_image_grid(images, rows=2, cols=4)\n</code></pre> <p>Whereas before you couldn't even generate a batch of 4 images, now you can generate a batch of 8 images at ~3.5 seconds per image! This is probably the fastest you can go on a T4 GPU without sacrificing quality.</p>"},{"location":"diffusers/stable_diffusion/#quality","title":"Quality","text":"<p>In the last two sections, you learned how to optimize the speed of your pipeline by using <code>fp16</code>, reducing the number of inference steps by using a more performant scheduler, and enabling attention slicing to reduce memory consumption. Now you're going to focus on how to improve the quality of generated images.</p>"},{"location":"diffusers/stable_diffusion/#better-checkpoints","title":"Better checkpoints","text":"<p>The most obvious step is to use better checkpoints. The Stable Diffusion model is a good starting point, and since its official launch, several improved versions have also been released. However, using a newer version doesn't automatically mean you'll get better results. You'll still have to experiment with different checkpoints yourself, and do a little research (such as using negative prompts) to get the best results.</p> <p>As the field grows, there are more and more high-quality checkpoints finetuned to produce certain styles. Try exploring the Hub and Diffusers Gallery to find one you're interested in!</p>"},{"location":"diffusers/stable_diffusion/#better-pipeline-components","title":"Better pipeline components","text":"<p>You can also try replacing the current pipeline components with a newer version. Let's try loading the latest autoencoder from Stability AI into the pipeline, and generate some images:</p> <pre><code>from mindone.diffusers import AutoencoderKL\n\nvae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-mse\", mindspore_dtype=mindspore.float16).to(\"cuda\")\npipeline.vae = vae\nimages = pipeline(**get_inputs(batch_size=8))[0]\nmake_image_grid(images, rows=2, cols=4)\n</code></pre>"},{"location":"diffusers/stable_diffusion/#better-prompt-engineering","title":"Better prompt engineering","text":"<p>The text prompt you use to generate an image is super important, so much so that it is called prompt engineering. Some considerations to keep during prompt engineering are:</p> <ul> <li>How is the image or similar images of the one I want to generate stored on the internet?</li> <li>What additional detail can I give that steers the model towards the style I want?</li> </ul> <p>With this in mind, let's improve the prompt to include color and higher quality details:</p> <pre><code>prompt += \", tribal panther make up, blue on red, side profile, looking away, serious eyes\"\nprompt += \" 50mm portrait photography, hard rim lighting photography--beta --ar 2:3  --beta --upbeta\"\n</code></pre> <p>Generate a batch of images with the new prompt:</p> <pre><code>images = pipeline(**get_inputs(batch_size=8))[0]\nmake_image_grid(images, rows=2, cols=4)\n</code></pre> <p>Pretty impressive! Let's tweak the second image - corresponding to the <code>Generator</code> with a seed of <code>1</code> - a bit more by adding some text about the age of the subject:</p> <pre><code>prompts = [\n    \"portrait photo of the oldest warrior chief, tribal panther make up, blue on red, side profile, looking away, serious eyes 50mm portrait photography, hard rim lighting photography--beta --ar 2:3  --beta --upbeta\",\n    \"portrait photo of a old warrior chief, tribal panther make up, blue on red, side profile, looking away, serious eyes 50mm portrait photography, hard rim lighting photography--beta --ar 2:3  --beta --upbeta\",\n    \"portrait photo of a warrior chief, tribal panther make up, blue on red, side profile, looking away, serious eyes 50mm portrait photography, hard rim lighting photography--beta --ar 2:3  --beta --upbeta\",\n    \"portrait photo of a young warrior chief, tribal panther make up, blue on red, side profile, looking away, serious eyes 50mm portrait photography, hard rim lighting photography--beta --ar 2:3  --beta --upbeta\",\n]\n\ngenerator = [np.random.Generator(np.random.PCG64(seed=1)) for _ in range(len(prompts))]\nimages = pipeline(prompt=prompts, generator=generator, num_inference_steps=25)[0]\nmake_image_grid(images, 2, 2)\n</code></pre>"},{"location":"diffusers/stable_diffusion/#next-steps","title":"Next steps","text":"<p>In this tutorial, you learned how to optimize a [<code>DiffusionPipeline</code>] for computational and memory efficiency as well as improving the quality of generated outputs. If you're interested in making your pipeline even faster, take a look at the following resources:</p> <ul> <li>Learn how PyTorch 2.0 and <code>torch.compile</code> can yield 5 - 300% faster inference speed. On an A100 GPU, inference can be up to 50% faster!</li> <li>If you can't use PyTorch 2, we recommend you install xFormers. Its memory-efficient attention mechanism works great with PyTorch 1.13.1 for faster speed and reduced memory consumption.</li> <li>Other optimization techniques, such as model offloading, are covered in this guide.</li> </ul>"},{"location":"diffusers/tutorials/basic_training/","title":"Train a diffusion model","text":""},{"location":"diffusers/tutorials/basic_training/#train-a-diffusion-model","title":"Train a diffusion model","text":"<p>Unconditional image generation is a popular application of diffusion models that generates images that look like those in the dataset used for training. Typically, the best results are obtained from finetuning a pretrained model on a specific dataset. You can find many of these checkpoints on the Hub, but if you can't find one you like, you can always train your own!</p> <p>This tutorial will teach you how to train a [<code>UNet2DModel</code>] from scratch on a subset of the Smithsonian Butterflies dataset to generate your own \ud83e\udd8b butterflies \ud83e\udd8b.</p> <p>Tip</p> <p>\ud83d\udca1 This training tutorial is based on the Training with \ud83e\udde8 Diffusers notebook. For additional details and context about diffusion models like how they work, check out the notebook!</p> <p>Before you begin, make sure you have \ud83e\udd17 Datasets installed to load and preprocess image datasets, and \ud83e\udd17 Accelerate, to simplify training on any number of GPUs. The following command will also install TensorBoard to visualize training metrics (you can also use Weights &amp; Biases to track your training).</p> <pre><code># uncomment to install the necessary libraries in Colab\n#!pip install mindone[training]\n</code></pre> <p>We encourage you to share your model with the community, and in order to do that, you'll need to login to your Hugging Face account (create one here if you don't already have one!). You can login from a notebook and enter your token when prompted. Make sure your token has the write role.</p> <pre><code>&gt;&gt;&gt; from huggingface_hub import notebook_login\n\n&gt;&gt;&gt; notebook_login()\n</code></pre> <p>Or login in from the terminal:</p> <pre><code>huggingface-cli login\n</code></pre> <p>Since the model checkpoints are quite large, install Git-LFS to version these large files:</p> <pre><code>!sudo apt -qq install git-lfs\n!git config --global credential.helper store\n</code></pre>"},{"location":"diffusers/tutorials/basic_training/#training-configuration","title":"Training configuration","text":"<p>For convenience, create a <code>TrainingConfig</code> class containing the training hyperparameters (feel free to adjust them):</p> <pre><code>&gt;&gt;&gt; from dataclasses import dataclass\n\n&gt;&gt;&gt; @dataclass\n... class TrainingConfig:\n...     image_size = 128  # the generated image resolution\n...     train_batch_size = 16\n...     eval_batch_size = 16  # how many images to sample during evaluation\n...     num_epochs = 50\n...     gradient_accumulation_steps = 1\n...     learning_rate = 1e-4\n...     lr_warmup_steps = 500\n...     save_image_epochs = 10\n...     save_model_epochs = 30\n...     mixed_precision = \"fp16\"  # `no` for float32, `fp16` for automatic mixed precision\n...     output_dir = \"ddpm-butterflies-128\"  # the model name locally and on the HF Hub\n...\n...     push_to_hub = True  # whether to upload the saved model to the HF Hub\n...     hub_model_id = \"&lt;your-username&gt;/&lt;my-awesome-model&gt;\"  # the name of the repository to create on the HF Hub\n...     hub_private_repo = False\n...     overwrite_output_dir = True  # overwrite the old model when re-running the notebook\n...     seed = 0\n\n&gt;&gt;&gt; config = TrainingConfig()\n</code></pre>"},{"location":"diffusers/tutorials/basic_training/#load-the-dataset","title":"Load the dataset","text":"<p>You can easily load the Smithsonian Butterflies dataset with the \ud83e\udd17 Datasets library:</p> <pre><code>&gt;&gt;&gt; from datasets import load_dataset\n\n&gt;&gt;&gt; config.dataset_name = \"huggan/smithsonian_butterflies_subset\"\n&gt;&gt;&gt; dataset = load_dataset(config.dataset_name, split=\"train\")\n</code></pre> <p>Tip</p> <p>\ud83d\udca1 You can find additional datasets from the HugGan Community Event or you can use your own dataset by creating a local <code>ImageFolder</code>. Set <code>config.dataset_name</code> to the repository id of the dataset if it is from the HugGan Community Event, or <code>imagefolder</code> if you're using your own images.</p> <p>\ud83e\udd17 Datasets uses the [<code>~datasets.Image</code>] feature to automatically decode the image data and load it as a <code>PIL.Image</code> which we can visualize:</p> <pre><code>&gt;&gt;&gt; import matplotlib.pyplot as plt\n\n&gt;&gt;&gt; fig, axs = plt.subplots(1, 4, figsize=(16, 4))\n&gt;&gt;&gt; for i, image in enumerate(dataset[:4][\"image\"]):\n...     axs[i].imshow(image)\n...     axs[i].set_axis_off()\n&gt;&gt;&gt; fig.show()\n</code></pre> <p>The images are all different sizes though, so you'll need to preprocess them first:</p> <ul> <li><code>Resize</code> changes the image size to the one defined in <code>config.image_size</code>.</li> <li><code>RandomHorizontalFlip</code> augments the dataset by randomly mirroring the images.</li> <li><code>Normalize</code> is important to rescale the pixel values into a [-1, 1] range, which is what the model expects.</li> </ul> <pre><code>&gt;&gt;&gt; from mindspore.dataset import transforms, vision\n\n&gt;&gt;&gt; preprocess = transforms.Compose(\n...     [\n...         vision.Resize((config.image_size, config.image_size)),\n...         vision.RandomHorizontalFlip(),\n...         vision.ToTensor(),\n...         vision.Normalize([0.5], [0.5], is_hwc=False),\n...     ]\n... )\n</code></pre> <p>Use \ud83e\udd17 Datasets' [<code>~datasets.Dataset.set_transform</code>] method to apply the <code>preprocess</code> function on the fly during training:</p> <pre><code>&gt;&gt;&gt; def transform(examples):\n...     images = [preprocess(image.convert(\"RGB\"))[0] for image in examples[\"image\"]]\n...     return {\"images\": images}\n\n\n&gt;&gt;&gt; dataset.set_transform(transform)\n</code></pre> <p>Feel free to visualize the images again to confirm that they've been resized. Now you're ready to wrap the dataset in a DataLoader for training!</p> <pre><code>&gt;&gt;&gt; from mindspore.dataset import GeneratorDataset\n\n&gt;&gt;&gt; class DatasetForMindData:\n...     def __init__(self, data):\n...         self.data = data\n...\n...     def __getitem__(self, idx):\n...         idx = idx.item() if isinstance(idx, np.integer) else idx\n...         return np.array(self.data[idx][\"images\"], dtype=np.float32)\n...\n...     def __len__(self):\n...         return len(self.data)\n\n&gt;&gt;&gt; train_dataloader = GeneratorDataset(DatasetForMindData(dataset), batch_size=config.train_batch_size, shuffle=True)\n</code></pre>"},{"location":"diffusers/tutorials/basic_training/#create-a-unet2dmodel","title":"Create a UNet2DModel","text":"<p>Pretrained models in \ud83e\udde8 Diffusers are easily created from their model class with the parameters you want. For example, to create a [<code>UNet2DModel</code>]:</p> <pre><code>&gt;&gt;&gt; from mindone.diffusers import UNet2DModel\n\n&gt;&gt;&gt; model = UNet2DModel(\n...     sample_size=config.image_size,  # the target image resolution\n...     in_channels=3,  # the number of input channels, 3 for RGB images\n...     out_channels=3,  # the number of output channels\n...     layers_per_block=2,  # how many ResNet layers to use per UNet block\n...     block_out_channels=(128, 128, 256, 256, 512, 512),  # the number of output channels for each UNet block\n...     down_block_types=(\n...         \"DownBlock2D\",  # a regular ResNet downsampling block\n...         \"DownBlock2D\",\n...         \"DownBlock2D\",\n...         \"DownBlock2D\",\n...         \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n...         \"DownBlock2D\",\n...     ),\n...     up_block_types=(\n...         \"UpBlock2D\",  # a regular ResNet upsampling block\n...         \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n...         \"UpBlock2D\",\n...         \"UpBlock2D\",\n...         \"UpBlock2D\",\n...         \"UpBlock2D\",\n...     ),\n... )\n</code></pre> <p>It is often a good idea to quickly check the sample image shape matches the model output shape:</p> <pre><code>&gt;&gt;&gt; sample_image = dataset[0][\"images\"].unsqueeze(0)\n&gt;&gt;&gt; print(\"Input shape:\", sample_image.shape)\nInput shape: [1, 3, 128, 128]\n\n&gt;&gt;&gt; print(\"Output shape:\", model(mindspore.Tensor(sample_image), timestep=0)[0].shape)\nOutput shape: [1, 3, 128, 128]\n</code></pre> <p>Great! Next, you'll need a scheduler to add some noise to the image.</p>"},{"location":"diffusers/tutorials/basic_training/#create-a-scheduler","title":"Create a scheduler","text":"<p>The scheduler behaves differently depending on whether you're using the model for training or inference. During inference, the scheduler generates image from the noise. During training, the scheduler takes a model output - or a sample - from a specific point in the diffusion process and applies noise to the image according to a noise schedule and an update rule.</p> <p>Let's take a look at the [<code>DDPMScheduler</code>] and use the <code>add_noise</code> method to add some random noise to the <code>sample_image</code> from before:</p> <pre><code>&gt;&gt;&gt; import mindspore\n&gt;&gt;&gt; from PIL import Image\n&gt;&gt;&gt; from mindone.diffusers import DDPMScheduler\n\n&gt;&gt;&gt; noise_scheduler = DDPMScheduler(num_train_timesteps=1000)\n&gt;&gt;&gt; noise = mindspore.ops.randn(sample_image.shape)\n&gt;&gt;&gt; timesteps = mindspore.Tensor([50])\n&gt;&gt;&gt; noisy_image = noise_scheduler.add_noise(sample_image, noise, timesteps)\n\n&gt;&gt;&gt; Image.fromarray(((noisy_image.permute(0, 2, 3, 1) + 1.0) * 127.5).type(mindspore.uint8).numpy()[0])\n</code></pre> <p>The training objective of the model is to predict the noise added to the image. The loss at this step can be calculated by:</p> <pre><code>&gt;&gt;&gt; from mindspore import ops\n\n&gt;&gt;&gt; noise_pred = model(noisy_image, timesteps)[0]\n&gt;&gt;&gt; loss = ops.mse_loss(noise_pred, noise)\n</code></pre>"},{"location":"diffusers/tutorials/basic_training/#train-the-model","title":"Train the model","text":"<p>By now, you have most of the pieces to start training the model and all that's left is putting everything together.</p> <p>First, you'll need an optimizer and a learning rate scheduler:</p> <pre><code>&gt;&gt;&gt; from mindspore import nn\n&gt;&gt;&gt; from mindone.diffusers.optimization import get_cosine_schedule_with_warmup\n\n&gt;&gt;&gt; lr_scheduler = get_cosine_schedule_with_warmup(\n...     config.learning_rate\n...     num_warmup_steps=config.lr_warmup_steps,\n...     num_training_steps=(len(train_dataloader) * config.num_epochs),\n... )\n&gt;&gt;&gt; optimizer = nn.AdamWeightDecay(model.trainable_params(), learning_rate=lr_scheduler)\n</code></pre> <p>Then, you'll need a way to evaluate the model. For evaluation, you can use the [<code>DDPMPipeline</code>] to generate a batch of sample images and save it as a grid:</p> <pre><code>&gt;&gt;&gt;import numpy as np from mindone.diffusers import DDPMPipeline\n&gt;&gt;&gt; from mindone.diffusers.utils import make_image_grid\n&gt;&gt;&gt; import os\n\n&gt;&gt;&gt; def evaluate(config, epoch, pipeline):\n...     # Sample some images from random noise (this is the backward diffusion process).\n...     # The default pipeline output type is `List[PIL.Image]`\n...     images = pipeline(\n...         batch_size=config.eval_batch_size,\n...         generator=np.random.Generator(np.random.PCG64(config.seed)),\n...     )[0]\n...\n...     # Make a grid out of the images\n...     image_grid = make_image_grid(images, rows=4, cols=4)\n...\n...     # Save the images\n...     test_dir = os.path.join(config.output_dir, \"samples\")\n...     os.makedirs(test_dir, exist_ok=True)\n...     image_grid.save(f\"{test_dir}/{epoch:04d}.png\")\n</code></pre> <p>Now you can wrap all these components together in a training loop with TensorBoard logging, gradient accumulation, and mixed precision training. To upload the model to the Hub, write a function to get your repository name and information and then push it to the Hub.</p> <p>Tip</p> <p>\ud83d\udca1 The training loop below may look intimidating and long, but it'll be worth it later when you launch your training in just one line of code! If you can't wait and want to start generating images, feel free to copy and run the code below. You can always come back and examine the training loop more closely later, like when you're waiting for your model to finish training. \ud83e\udd17</p> <pre><code>&gt;&gt;&gt; from huggingface_hub import create_repo, upload_folder\n&gt;&gt;&gt; from tqdm.auto import tqdm\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; import os\n&gt;&gt;&gt; from mindone.diffusers.training_utils import TrainStep\n\n&gt;&gt;&gt; # Write your train step\n&gt;&gt;&gt; class MyTrainStep(TrainStep):\n...     def __init__(\n...         self,\n...         model: nn.Cell,\n...         optimizer: nn.Optimizer,\n...         noise_scheduler,\n...         gradient_accumulation_steps,\n...         length_of_dataloader,\n...     ):\n...         super().__init__(\n...             model,\n...             optimizer,\n...             StaticLossScaler(65536),\n...             1.0,\n...             gradient_accumulation_steps,\n...             gradient_accumulation_kwargs=dict(length_of_dataloader=length_of_dataloader),\n...         )\n...         self.model = model\n...         self.noise_scheduler = noise_scheduler\n...         self.noise_scheduler_num_train_timesteps = noise_scheduler.config.num_train_timesteps\n...\n...     def forward(self, clean_images):\n...         # Sample noise to add to the images\n...         noise = ops.randn(clean_images.shape)\n...         bs = clean_images.shape[0]\n...\n...         # Sample a random timestep for each image\n...         timesteps = ops.randint(\n...             0, noise_scheduler_num_train_timesteps, (bs,), dtype=mindspore.int64\n...         )\n...\n...         # Add noise to the clean images according to the noise magnitude at each timestep\n...         # (this is the forward diffusion process)\n...         noisy_images = self.noise_scheduler.add_noise(clean_images, noise, timesteps)\n...\n...         # Predict the noise residual\n...         noise_pred = self.model(noisy_images, timesteps, return_dict=False)[0]\n...         loss = ops.mse_loss(noise_pred, noise)\n...         loss = self.scale_loss(loss)\n...         return loss, noise_pred\n\n&gt;&gt;&gt; is_main_process, is_local_main_process = True, True\n&gt;&gt;&gt; train_step = MyTrainStep(model, optimizer, noise_scheduler, config.gradient_accumulation_steps, len(train_dataloader))\n&gt;&gt;&gt; pipeline = DDPMPipeline(unet=model, scheduler=noise_scheduler)\n\n&gt;&gt;&gt; def train_loop(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler):\n...     if is_main_process:\n...         if config.output_dir is not None:\n...             os.makedirs(config.output_dir, exist_ok=True)\n...         if config.push_to_hub:\n...             repo_id = create_repo(\n...                 repo_id=config.hub_model_id or Path(config.output_dir).name, exist_ok=True\n...             ).repo_id\n...\n...     global_step = 0\n...\n...     # Now you train the model\n...     for epoch in range(config.num_epochs):\n...         progress_bar = tqdm(total=len(train_dataloader), disable=not is_local_main_process)\n...         progress_bar.set_description(f\"Epoch {epoch}\")\n...\n...         for step, batch in enumerate(train_dataloader):\n...             loss, model_pred = train_step(*batch)\n...\n...             progress_bar.update(1)\n...             logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step}\n...             progress_bar.set_postfix(**logs)\n...             accelerator.log(logs, step=global_step)\n...             global_step += 1\n...\n...         # After each epoch you optionally sample some demo images with evaluate() and save the model\n...         if is_main_process:\n...             if (epoch + 1) % config.save_image_epochs == 0 or epoch == config.num_epochs - 1:\n...                 evaluate(config, epoch, pipeline)\n...\n...             if (epoch + 1) % config.save_model_epochs == 0 or epoch == config.num_epochs - 1:\n...                 if config.push_to_hub:\n...                     upload_folder(\n...                         repo_id=repo_id,\n...                         folder_path=config.output_dir,\n...                         commit_message=f\"Epoch {epoch}\",\n...                         ignore_patterns=[\"step_*\", \"epoch_*\"],\n...                     )\n...                 else:\n...                     pipeline.save_pretrained(config.output_dir)\n</code></pre> <p>If you want to launch a distributed training, see tutorial from mindspore. And you can get the rank of process by:</p> <pre><code>&gt;&gt;&gt; from mindspore.communication import get_local_rank, get_rank\n&gt;&gt;&gt; rank, local_rank = get_rank(), get_local_rank()\n&gt;&gt;&gt; is_main_process, is_local_main_process = rank == 0, local_rank == 0\n</code></pre> <p>Once training is complete, take a look at the final \ud83e\udd8b images \ud83e\udd8b generated by your diffusion model!</p> <pre><code>&gt;&gt;&gt; import glob\n\n&gt;&gt;&gt; sample_images = sorted(glob.glob(f\"{config.output_dir}/samples/*.png\"))\n&gt;&gt;&gt; Image.open(sample_images[-1])\n</code></pre>"},{"location":"diffusers/tutorials/basic_training/#next-steps","title":"Next steps","text":"<p>Unconditional image generation is one example of a task that can be trained. You can explore other tasks and training techniques by visiting the \ud83e\udde8 Diffusers Training Examples page. Here are some examples of what you can learn:</p> <ul> <li>Textual Inversion, an algorithm that teaches a model a specific visual concept and integrates it into the generated image.</li> <li>DreamBooth, a technique for generating personalized images of a subject given several input images of the subject.</li> <li>Guide to finetuning a Stable Diffusion model on your own dataset.</li> <li>Guide to using LoRA, a memory-efficient technique for finetuning really large models faster.</li> </ul>"},{"location":"diffusers/tutorials/tutorial_overview/","title":"Overview","text":""},{"location":"diffusers/tutorials/tutorial_overview/#overview","title":"Overview","text":"<p>Welcome to \ud83e\udde8 Diffusers! If you're new to diffusion models and generative AI, and want to learn more, then you've come to the right place. These beginner-friendly tutorials are designed to provide a gentle introduction to diffusion models and help you understand the library fundamentals - the core components and how \ud83e\udde8 Diffusers is meant to be used.</p> <p>You'll learn how to use a pipeline for inference to rapidly generate things, and then deconstruct that pipeline to really understand how to use the library as a modular toolbox for building your own diffusion systems. In the next lesson, you'll learn how to train your own diffusion model to generate what you want.</p> <p>After completing the tutorials, you'll have gained the necessary skills to start exploring the library on your own and see how to use it for your own projects and applications.</p> <p>Feel free to join our community on Discord or the forums to connect and collaborate with other users and developers!</p> <p>Let's start diffusing! \ud83e\udde8</p>"},{"location":"diffusers/tutorials/using_peft_for_inference/","title":"Load LoRAs for inference","text":""},{"location":"diffusers/tutorials/using_peft_for_inference/#load-loras-for-inference","title":"Load LoRAs for inference","text":"<p>There are many adapter types (with LoRAs being the most popular) trained in different styles to achieve different effects. You can even combine multiple adapters to create new and unique images.</p> <p>In this tutorial, you'll learn how to easily load and manage adapters for inference with the \ud83e\udd17 PEFT integration in \ud83e\udd17 Diffusers. You'll use LoRA as the main adapter technique, so you'll see the terms LoRA and adapter used interchangeably.</p> <p>Let's first install all the required libraries.</p> <pre><code>!pip install transformers mindone\n</code></pre> <p>Now, load a pipeline with a Stable Diffusion XL (SDXL) checkpoint:</p> <pre><code>from mindone.diffusers import DiffusionPipeline\nimport mindspore\nimport numpy as np\n\npipe_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\npipe = DiffusionPipeline.from_pretrained(pipe_id, mindspore_dtype=mindspore.float16)\n</code></pre> <p>Next, load a CiroN2022/toy-face adapter with the [<code>~diffusers.loaders.StableDiffusionXLLoraLoaderMixin.load_lora_weights</code>] method. With the \ud83e\udd17 PEFT integration, you can assign a specific <code>adapter_name</code> to the checkpoint, which let's you easily switch between different LoRA checkpoints. Let's call this adapter <code>\"toy\"</code>.</p> <pre><code>pipe.load_lora_weights(\"CiroN2022/toy-face\", weight_name=\"toy_face_sdxl.safetensors\", adapter_name=\"toy\")\n</code></pre> <p>Make sure to include the token <code>toy_face</code> in the prompt and then you can perform inference:</p> <pre><code>prompt = \"toy_face of a hacker with a hoodie\"\n\nlora_scale= 0.9\nimage = pipe(\n    prompt, num_inference_steps=30, cross_attention_kwargs={\"scale\": lora_scale}, generator=np.random.Generator(np.random.PCG64(0))\n)[0][0]\nimage\n</code></pre> <p></p> <p>With the <code>adapter_name</code> parameter, it is really easy to use another adapter for inference! Load the nerijs/pixel-art-xl adapter that has been fine-tuned to generate pixel art images and call it <code>\"pixel\"</code>.</p> <p>The pipeline automatically sets the first loaded adapter (<code>\"toy\"</code>) as the active adapter, but you can activate the <code>\"pixel\"</code> adapter with the [<code>~diffusers.loaders.UNet2DConditionLoadersMixin.set_adapters</code>] method:</p> <pre><code>pipe.load_lora_weights(\"nerijs/pixel-art-xl\", weight_name=\"pixel-art-xl.safetensors\", adapter_name=\"pixel\")\npipe.set_adapters(\"pixel\")\n</code></pre> <p>Make sure you include the token <code>pixel art</code> in your prompt to generate a pixel art image:</p> <pre><code>prompt = \"a hacker with a hoodie, pixel art\"\nimage = pipe(\n    prompt, num_inference_steps=30, cross_attention_kwargs={\"scale\": lora_scale}, generator=np.random.Generator(np.random.PCG64(0))\n)[0][0]\nimage\n</code></pre> <p></p>"},{"location":"diffusers/tutorials/using_peft_for_inference/#merge-adapters","title":"Merge adapters","text":"<p>You can also merge different adapter checkpoints for inference to blend their styles together.</p> <p>Once again, use the [<code>~diffusers.loaders.UNet2DConditionLoadersMixin.set_adapters</code>] method to activate the <code>pixel</code> and <code>toy</code> adapters and specify the weights for how they should be merged.</p> <pre><code>pipe.set_adapters([\"pixel\", \"toy\"], adapter_weights=[0.5, 1.0])\n</code></pre> <p>Tip</p> <p>LoRA checkpoints in the diffusion community are almost always obtained with DreamBooth. DreamBooth training often relies on \"trigger\" words in the input text prompts in order for the generation results to look as expected. When you combine multiple LoRA checkpoints, it's important to ensure the trigger words for the corresponding LoRA checkpoints are present in the input text prompts.</p> <p>Remember to use the trigger words for CiroN2022/toy-face and nerijs/pixel-art-xl (these are found in their repositories) in the prompt to generate an image.</p> <pre><code>prompt = \"toy_face of a hacker with a hoodie, pixel art\"\nimage = pipe(\n    prompt, num_inference_steps=30, cross_attention_kwargs={\"scale\": 1.0}, generator=torch.manual_seed(0)\n).images[0]\nimage\n</code></pre> <p></p> <p>Impressive! As you can see, the model generated an image that mixed the characteristics of both adapters.</p> <p>Tip</p> <p>Through its PEFT integration, Diffusers also offers more efficient merging methods which you can learn about in the Merge LoRAs guide!</p> <p>To return to only using one adapter, use the [<code>~diffusers.loaders.UNet2DConditionLoadersMixin.set_adapters</code>] method to activate the <code>\"toy\"</code> adapter:</p> <pre><code>pipe.set_adapters(\"toy\")\n\nprompt = \"toy_face of a hacker with a hoodie\"\nlora_scale= 0.9\nimage = pipe(\n    prompt, num_inference_steps=30, cross_attention_kwargs={\"scale\": lora_scale}, generator=np.random.Generator(np.random.PCG64(0))\n)[0][0]\nimage\n</code></pre> <p>Or to disable all adapters entirely, use the [<code>~diffusers.loaders.UNet2DConditionLoadersMixin.disable_lora</code>] method to return the base model.</p> <pre><code>pipe.disable_lora()\n\nprompt = \"toy_face of a hacker with a hoodie\"\nlora_scale= 0.9\nimage = pipe(prompt, num_inference_steps=30, generator=np.random.Generator(np.random.PCG64(0)))[0][0]\nimage\n</code></pre>"},{"location":"diffusers/tutorials/using_peft_for_inference/#manage-active-adapters","title":"Manage active adapters","text":"<p>You have attached multiple adapters in this tutorial, and if you're feeling a bit lost on what adapters have been attached to the pipeline's components, use the [<code>~diffusers.loaders.LoraLoaderMixin.get_active_adapters</code>] method to check the list of active adapters:</p> <pre><code>active_adapters = pipe.get_active_adapters()\nactive_adapters\n[\"toy\", \"pixel\"]\n</code></pre> <p>You can also get the active adapters of each pipeline component with [<code>~diffusers.loaders.LoraLoaderMixin.get_list_adapters</code>]:</p> <pre><code>list_adapters_component_wise = pipe.get_list_adapters()\nlist_adapters_component_wise\n{\"text_encoder\": [\"toy\", \"pixel\"], \"unet\": [\"toy\", \"pixel\"], \"text_encoder_2\": [\"toy\", \"pixel\"]}\n</code></pre>"},{"location":"diffusers/using-diffusers/write_own_pipeline/","title":"Understanding pipelines, models and schedulers","text":""},{"location":"diffusers/using-diffusers/write_own_pipeline/#understanding-pipelines-models-and-schedulers","title":"Understanding pipelines, models and schedulers","text":"<p>\ud83e\udde8 Diffusers is designed to be a user-friendly and flexible toolbox for building diffusion systems tailored to your use-case. At the core of the toolbox are models and schedulers. While the [<code>DiffusionPipeline</code>] bundles these components together for convenience, you can also unbundle the pipeline and use the models and schedulers separately to create new diffusion systems.</p> <p>In this tutorial, you'll learn how to use models and schedulers to assemble a diffusion system for inference, starting with a basic pipeline and then progressing to the Stable Diffusion pipeline.</p>"},{"location":"diffusers/using-diffusers/write_own_pipeline/#deconstruct-a-basic-pipeline","title":"Deconstruct a basic pipeline","text":"<p>A pipeline is a quick and easy way to run a model for inference, requiring no more than four lines of code to generate an image:</p> <pre><code>&gt;&gt;&gt; from mindone.diffusers import DDPMPipeline\n\n&gt;&gt;&gt; ddpm = DDPMPipeline.from_pretrained(\"google/ddpm-cat-256\", use_safetensors=True)\n&gt;&gt;&gt; image = ddpm(num_inference_steps=25)[0][0]\n&gt;&gt;&gt; image\n</code></pre> <p>That was super easy, but how did the pipeline do that? Let's breakdown the pipeline and take a look at what's happening under the hood.</p> <p>In the example above, the pipeline contains a [<code>UNet2DModel</code>] model and a [<code>DDPMScheduler</code>]. The pipeline denoises an image by taking random noise the size of the desired output and passing it through the model several times. At each timestep, the model predicts the noise residual and the scheduler uses it to predict a less noisy image. The pipeline repeats this process until it reaches the end of the specified number of inference steps.</p> <p>To recreate the pipeline with the model and scheduler separately, let's write our own denoising process.</p> <ol> <li> <p>Load the model and scheduler:</p> <pre><code>&gt;&gt;&gt; from mindone.diffusers import DDPMScheduler, UNet2DModel\n\n&gt;&gt;&gt; scheduler = DDPMScheduler.from_pretrained(\"google/ddpm-cat-256\")\n&gt;&gt;&gt; model = UNet2DModel.from_pretrained(\"google/ddpm-cat-256\", use_safetensors=True)\n</code></pre> </li> <li> <p>Set the number of timesteps to run the denoising process for:</p> <pre><code>&gt;&gt;&gt; scheduler.set_timesteps(50)\n</code></pre> </li> <li> <p>Setting the scheduler timesteps creates a tensor with evenly spaced elements in it, 50 in this example. Each element corresponds to a timestep at which the model denoises an image. When you create the denoising loop later, you'll iterate over this tensor to denoise an image:</p> <pre><code>&gt;&gt;&gt; scheduler.timesteps\ntensor([980, 960, 940, 920, 900, 880, 860, 840, 820, 800, 780, 760, 740, 720,\n    700, 680, 660, 640, 620, 600, 580, 560, 540, 520, 500, 480, 460, 440,\n    420, 400, 380, 360, 340, 320, 300, 280, 260, 240, 220, 200, 180, 160,\n    140, 120, 100,  80,  60,  40,  20,   0])\n</code></pre> </li> <li> <p>Create some random noise with the same shape as the desired output:</p> <pre><code>&gt;&gt;&gt; import mindspore\n\n&gt;&gt;&gt; sample_size = model.config.sample_size\n&gt;&gt;&gt; noise = mindspore.ops.randn((1, 3, sample_size, sample_size))\n</code></pre> </li> <li> <p>Now write a loop to iterate over the timesteps. At each timestep, the model does a [<code>UNet2DModel.forward</code>] pass and returns the noisy residual. The scheduler's [<code>~DDPMScheduler.step</code>] method takes the noisy residual, timestep, and input and it predicts the image at the previous timestep. This output becomes the next input to the model in the denoising loop, and it'll repeat until it reaches the end of the <code>timesteps</code> array.</p> <pre><code>&gt;&gt;&gt; input = noise\n\n&gt;&gt;&gt; for t in scheduler.timesteps:\n...     noisy_residual = model(input, t)[0]\n...     previous_noisy_sample = scheduler.step(noisy_residual, t, input)[0]\n...     input = previous_noisy_sample\n</code></pre> <p>This is the entire denoising process, and you can use this same pattern to write any diffusion system.</p> </li> <li> <p>The last step is to convert the denoised output into an image:</p> <pre><code>&gt;&gt;&gt; from PIL import Image\n&gt;&gt;&gt; import numpy as np\n\n&gt;&gt;&gt; image = (input / 2 + 0.5).clamp(0, 1).squeeze()\n&gt;&gt;&gt; image = (image.permute(1, 2, 0) * 255).round().to(mindspore.uint8).numpy()\n&gt;&gt;&gt; image = Image.fromarray(image)\n&gt;&gt;&gt; image\n</code></pre> </li> </ol> <p>In the next section, you'll put your skills to the test and breakdown the more complex Stable Diffusion pipeline. The steps are more or less the same. You'll initialize the necessary components, and set the number of timesteps to create a <code>timestep</code> array. The <code>timestep</code> array is used in the denoising loop, and for each element in this array, the model predicts a less noisy image. The denoising loop iterates over the <code>timestep</code>'s, and at each timestep, it outputs a noisy residual and the scheduler uses it to predict a less noisy image at the previous timestep. This process is repeated until you reach the end of the <code>timestep</code> array.</p> <p>Let's try it out!</p>"},{"location":"diffusers/using-diffusers/write_own_pipeline/#deconstruct-the-stable-diffusion-pipeline","title":"Deconstruct the Stable Diffusion pipeline","text":"<p>Stable Diffusion is a text-to-image latent diffusion model. It is called a latent diffusion model because it works with a lower-dimensional representation of the image instead of the actual pixel space, which makes it more memory efficient. The encoder compresses the image into a smaller representation, and a decoder to convert the compressed representation back into an image. For text-to-image models, you'll need a tokenizer and an encoder to generate text embeddings. From the previous example, you already know you need a UNet model and a scheduler.</p> <p>As you can see, this is already more complex than the DDPM pipeline which only contains a UNet model. The Stable Diffusion model has three separate pretrained models.</p> <p>Tip</p> <p>\ud83d\udca1 Read the How does Stable Diffusion work? blog for more details about how the VAE, UNet, and text encoder models work.</p> <p>Now that you know what you need for the Stable Diffusion pipeline, load all these components with the [<code>~ModelMixin.from_pretrained</code>] method. You can find them in the pretrained <code>runwayml/stable-diffusion-v1-5</code> checkpoint, and each component is stored in a separate subfolder:</p> <pre><code>&gt;&gt;&gt; from PIL import Image\n&gt;&gt;&gt; import mindspore\n&gt;&gt;&gt; from transformers import CLIPTokenizer\n&gt;&gt;&gt; from mindone.transformers import CLIPTextModel\n&gt;&gt;&gt; from mindone.diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler\n\n&gt;&gt;&gt; vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\", use_safetensors=True)\n&gt;&gt;&gt; tokenizer = CLIPTokenizer.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"tokenizer\")\n&gt;&gt;&gt; text_encoder = CLIPTextModel.from_pretrained(\n...     \"CompVis/stable-diffusion-v1-4\", subfolder=\"text_encoder\", use_safetensors=True\n... )\n&gt;&gt;&gt; unet = UNet2DConditionModel.from_pretrained(\n...     \"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", use_safetensors=True\n... )\n</code></pre> <p>Instead of the default [<code>PNDMScheduler</code>], exchange it for the [<code>UniPCMultistepScheduler</code>] to see how easy it is to plug a different scheduler in:</p> <pre><code>&gt;&gt;&gt; from mindone.diffusers import UniPCMultistepScheduler\n\n&gt;&gt;&gt; scheduler = UniPCMultistepScheduler.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\")\n</code></pre>"},{"location":"diffusers/using-diffusers/write_own_pipeline/#create-text-embeddings","title":"Create text embeddings","text":"<p>The next step is to tokenize the text to generate embeddings. The text is used to condition the UNet model and steer the diffusion process towards something that resembles the input prompt.</p> <p>Tip</p> <p>\ud83d\udca1 The <code>guidance_scale</code> parameter determines how much weight should be given to the prompt when generating an image.</p> <p>Feel free to choose any prompt you like if you want to generate something else!</p> <pre><code>&gt;&gt;&gt; prompt = [\"a photograph of an astronaut riding a horse\"]\n&gt;&gt;&gt; height = 512  # default height of Stable Diffusion\n&gt;&gt;&gt; width = 512  # default width of Stable Diffusion\n&gt;&gt;&gt; num_inference_steps = 25  # Number of denoising steps\n&gt;&gt;&gt; guidance_scale = 7.5  # Scale for classifier-free guidance\n&gt;&gt;&gt; generator = np.random.Generator(np.random.PCG64(seed=0))  # Seed generator to create the initial latent noise\n&gt;&gt;&gt; batch_size = len(prompt)\n</code></pre> <p>Tokenize the text and generate the embeddings from the prompt:</p> <pre><code>&gt;&gt;&gt; text_input = tokenizer(\n...     prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"np\"\n... )\n\n&gt;&gt;&gt; text_embeddings = text_encoder(mindspore.Tensor(text_input.input_ids))[0]\n</code></pre> <p>You'll also need to generate the unconditional text embeddings which are the embeddings for the padding token. These need to have the same shape (<code>batch_size</code> and <code>seq_length</code>) as the conditional <code>text_embeddings</code>:</p> <pre><code>&gt;&gt;&gt; max_length = text_input.input_ids.shape[-1]\n&gt;&gt;&gt; uncond_input = tokenizer([\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"np\")\n&gt;&gt;&gt; uncond_embeddings = text_encoder(mindspore.Tensor(uncond_input.input_ids))[0]\n</code></pre> <p>Let's concatenate the conditional and unconditional embeddings into a batch to avoid doing two forward passes:</p> <pre><code>&gt;&gt;&gt; text_embeddings = mindspore.ops.cat([uncond_embeddings, text_embeddings])\n</code></pre>"},{"location":"diffusers/using-diffusers/write_own_pipeline/#create-random-noise","title":"Create random noise","text":"<p>Next, generate some initial random noise as a starting point for the diffusion process. This is the latent representation of the image, and it'll be gradually denoised. At this point, the <code>latent</code> image is smaller than the final image size but that's okay though because the model will transform it into the final 512x512 image dimensions later.</p> <p>Tip</p> <p>\ud83d\udca1 The height and width are divided by 8 because the <code>vae</code> model has 3 down-sampling layers. You can check by running the following:</p> <pre><code>&gt;&gt;&gt; 2 ** (len(vae.config.block_out_channels) - 1) == 8\n</code></pre> <pre><code>&gt;&gt;&gt; latents = mindspore.ops.randn(\n...     (batch_size, unet.config.in_channels, height // 8, width // 8),\n... )\n</code></pre>"},{"location":"diffusers/using-diffusers/write_own_pipeline/#denoise-the-image","title":"Denoise the image","text":"<p>Start by scaling the input with the initial noise distribution, sigma, the noise scale value, which is required for improved schedulers like [<code>UniPCMultistepScheduler</code>]:</p> <pre><code>&gt;&gt;&gt; latents = latents * scheduler.init_noise_sigma\n</code></pre> <p>The last step is to create the denoising loop that'll progressively transform the pure noise in <code>latents</code> to an image described by your prompt. Remember, the denoising loop needs to do three things:</p> <ol> <li>Set the scheduler's timesteps to use during denoising.</li> <li>Iterate over the timesteps.</li> <li>At each timestep, call the UNet model to predict the noise residual and pass it to the scheduler to compute the previous noisy sample.</li> </ol> <pre><code>&gt;&gt;&gt; from tqdm.auto import tqdm\n\n&gt;&gt;&gt; scheduler.set_timesteps(num_inference_steps)\n\n&gt;&gt;&gt; for t in tqdm(scheduler.timesteps):\n...     # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n...     latent_model_input = mindspore.ops.cat([latents] * 2)\n...\n...     latent_model_input = scheduler.scale_model_input(latent_model_input, timestep=t)\n...\n...     # predict the noise residual\n...     noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings)[0]\n...\n...     # perform guidance\n...     noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n...     noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n...\n...     # compute the previous noisy sample x_t -&gt; x_t-1\n...     latents = scheduler.step(noise_pred, t, latents)[0]\n</code></pre>"},{"location":"diffusers/using-diffusers/write_own_pipeline/#decode-the-image","title":"Decode the image","text":"<p>The final step is to use the <code>vae</code> to decode the latent representation into an image and get the decoded output with <code>sample</code>:</p> <pre><code>&gt;&gt;&gt; # scale and decode the image latents with vae\n&gt;&gt;&gt; latents = 1 / 0.18215 * latents\n&gt;&gt;&gt; image = vae.decode(latents)[0]\n</code></pre> <p>Lastly, convert the image to a <code>PIL.Image</code> to see your generated image!</p> <pre><code>&gt;&gt;&gt; image = (image / 2 + 0.5).clamp(0, 1).squeeze()\n&gt;&gt;&gt; image = (image.permute(1, 2, 0) * 255).to(mindspore.uint8).numpy()\n&gt;&gt;&gt; image = Image.fromarray(image)\n&gt;&gt;&gt; image\n</code></pre>"},{"location":"diffusers/using-diffusers/write_own_pipeline/#next-steps","title":"Next steps","text":"<p>From basic to complex pipelines, you've seen that all you really need to write your own diffusion system is a denoising loop. The loop should set the scheduler's timesteps, iterate over them, and alternate between calling the UNet model to predict the noise residual and passing it to the scheduler to compute the previous noisy sample.</p> <p>This is really what \ud83e\udde8 Diffusers is designed for: to make it intuitive and easy to write your own diffusion system using models and schedulers.</p> <p>For your next steps, feel free to:</p> <ul> <li>Learn how to build and contribute a pipeline to \ud83e\udde8 Diffusers. We can't wait and see what you'll come up with!</li> <li>Explore existing pipelines in the library, and see if you can deconstruct and build a pipeline from scratch using the models and schedulers separately.</li> </ul>"},{"location":"peft/","title":"\ud83e\udd17 PEFT","text":"<p>Private preview stage. Working on it.</p> <p></p>"},{"location":"transformers/","title":"Get Pretrained Txt/Img Encoder from \ud83e\udd17 Transformers","text":"<p>This MindSpore patch for \ud83e\udd17 Transformers enables researchers or developers in the field of text-to-image (t2i) and text-to-video (t2v) generation to utilize pretrained text and image models from \ud83e\udd17 Transformers on MindSpore. The pretrained models from \ud83e\udd17 Transformers can be employed either as frozen encoders or fine-tuned with denoising networks for generative tasks. This approach aligns with the practices of PyTorch users<sup>[1][2]</sup>. Now, MindSpore users can benefit from the same functionality!</p>"},{"location":"transformers/#philosophy","title":"Philosophy","text":"<ul> <li>Only the MindSpore model definition will be implemented, which will be identical to the PyTorch model.</li> <li>Configuration, Tokenizer, etc. will utilize the original \ud83e\udd17 Transformers.</li> <li>Models here will be limited to the scope of generative tasks.</li> </ul>"}]}